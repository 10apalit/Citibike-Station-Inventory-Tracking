{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabd084f-cd0f-4973-88e0-c412bee4b8bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../final_project/includes/includes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f786ed83-105a-4db2-bb54-dbb20b1b97a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, count, to_timestamp,from_unixtime,split\n",
    "\n",
    "sample=spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/bronze_station_status.delta\")\n",
    "sample = sample.withColumn(\"datetime\", to_timestamp(from_unixtime(sample[\"last_reported\"])))\n",
    "\n",
    "sample = sample.withColumn(\"date\", sample[\"datetime\"].cast(\"date\"))\n",
    "sample = sample.withColumn(\"time\", split(sample[\"datetime\"].cast(\"string\"), \" \")[1])\n",
    "\n",
    "display(sample.filter(sample[\"date\"]=='2023-04-19'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa3ea5c3-afd6-4fb6-bf56-9025125a31ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore/tables/G06/silver/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28abe18a-01fc-46d4-a6dd-c3d91e7ab53f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample=sample.sort(sample[\"date\"].asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a706c9-ca26-4996-af3c-6134b465e32e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sample.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456ad08f-c74b-4a9a-81a7-b8a611424d7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "station_info=spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/bronze_station_info.delta\")\n",
    "display(station_info.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96218e43-c591-4fdf-b77e-61c4c67ff831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_live=spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/bronze_nyc_weather.delta\")\n",
    "display(weather_live.sort(weather_live[\"time\"].asc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7bb7511-1ef2-447f-bf78-5312fe702570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb02116-5de9-46ab-947c-2cfce2035f46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather=spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/G06/bronze/nyc_weather_history/\")\n",
    "display(weather.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3feece30-8ba8-4686-8d1f-53ddebf020e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q1 What are the monthly trip trends for your assigned station?\n",
    "\n",
    "from pyspark.sql.functions import year, month, count\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "# display(df.orderBy(\"started_at\").tail(5))\n",
    "df = df.withColumn(\"started_at\", df[\"started_at\"].cast(\"date\"))\n",
    "\n",
    "monthly_trips = df.groupBy(year(\"started_at\").alias(\"year\"), month(\"started_at\").alias(\"month\")) \\\n",
    "                  .agg(count(\"*\").alias(\"trips\")) \\\n",
    "                  .orderBy(\"year\", \"month\")\n",
    "\n",
    "display(monthly_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8739e6-b1e9-44c5-9dbe-4fa54fdf19ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Line chart for Monthly Trips Over Time\n",
    "from pyspark.sql.functions import concat, lit\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "monthly_trips = monthly_trips.withColumn(\"year_month\", \n",
    "                    concat(monthly_trips[\"year\"], lit(\"-\"), monthly_trips[\"month\"]))\n",
    "\n",
    "sns.lineplot(x=\"year_month\", y=\"trips\", data=monthly_trips.toPandas())\n",
    "\n",
    "plt.title(\"Monthly Trips Over Time\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "monthly_trips = monthly_trips.withColumn(\"year_month\", \n",
    "                    concat(monthly_trips[\"year\"], lit(\"-\"), monthly_trips[\"month\"]))\n",
    "\n",
    "fig = px.line(monthly_trips.toPandas(), x=\"year_month\", y=\"trips\")\n",
    "\n",
    "fig.update_layout(title=\"Monthly Trips Over Time\",\n",
    "                  xaxis_title=\"Year-Month\",\n",
    "                  yaxis_title=\"Trips\",\n",
    "                  xaxis_tickangle=-45)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9328ccf3-258b-4aa9-ab7a-ad2b843aaa66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bar Chart for Monthly Trips by Year\n",
    "\n",
    "sns.catplot(x=\"month\", y=\"trips\", hue=\"year\", kind=\"bar\", data=monthly_trips.toPandas(), height=6, aspect=2)\n",
    "\n",
    "plt.title(\"Monthly Trips by Year\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Trips\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce5b456-45ed-488e-8218-00b57a072872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Heat Map for Monthly Trips by Year\n",
    "\n",
    "pivot_table = monthly_trips.toPandas().pivot(\"month\", \"year\", \"trips\")\n",
    "\n",
    "sns.heatmap(pivot_table, cmap=\"YlGnBu\", annot=True, fmt=\".0f\")\n",
    "\n",
    "plt.title(\"Monthly Trips by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7666d7-f294-4ad0-a9b9-d5baad96891f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,desc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print('Number of rows: ', df.count())\n",
    "print('Number of columns: ', len(df.columns))\n",
    "\n",
    "print('Distinct rideable types', df.select('rideable_type').distinct().count())\n",
    "df.groupBy('rideable_type').count().show()\n",
    "print('Distinct end station names', df.select('end_station_name').distinct().count())\n",
    "df.groupBy('end_station_name').count().orderBy(desc('count')).show()\n",
    "print('Distinct membership types', df.select('member_casual').distinct().count())\n",
    "df.groupBy('member_casual').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260df107-5d35-4c6b-9697-da67148a45a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# Plot count of rideable types\n",
    "rideable_count = df_pd['rideable_type'].value_counts()\n",
    "fig = go.Figure([go.Bar(x=rideable_count.index, y=rideable_count.values, marker_color='blue')])\n",
    "fig.update_layout(title='Count of rideable types', xaxis_title='Rideable type', yaxis_title='Count')\n",
    "fig.show()\n",
    "\n",
    "# Plot count of membership types\n",
    "membership_count = df_pd['member_casual'].value_counts()\n",
    "fig = go.Figure([go.Bar(x=membership_count.index, y=membership_count.values, marker_color='green')])\n",
    "fig.update_layout(title='Count of membership types', xaxis_title='Membership type', yaxis_title='Count')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61eb4111-d350-44dd-af63-d940b1b2d631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get top 20 end stations\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "top_end_stations = df.groupBy('end_station_name').count().orderBy(desc('count')).limit(20)\n",
    "\n",
    "# create bar plot\n",
    "data = go.Bar(x=top_end_stations.select('end_station_name').rdd.flatMap(lambda x: x).collect(),\n",
    "              y=top_end_stations.select('count').rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "layout = go.Layout(title='Top 20 End Stations',\n",
    "                   xaxis=dict(title='End Station Name'),\n",
    "                   yaxis=dict(title='Count'))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bec5710b-785f-40c9-85f2-02863b537fad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rideable count types\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rideable_counts = df.groupBy('rideable_type').count().orderBy('count', ascending=False).toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(rideable_counts['rideable_type'], rideable_counts['count'])\n",
    "plt.title('Rideable Type Counts')\n",
    "plt.xlabel('Rideable Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "495dbf91-52aa-48bb-a271-5304e6f2d8ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hourly count of rides for the entire data\n",
    "\n",
    "from pyspark.sql.functions import hour\n",
    "\n",
    "hourly_counts = df.groupBy(hour('started_at').alias('hour')).count().orderBy('hour').toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(hourly_counts['hour'], hourly_counts['count'], marker='o')\n",
    "plt.title('Hourly Ride Counts')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Ride Count')\n",
    "plt.xticks(range(0,24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69b59a9-1761-4155-971e-50f6e654ca97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ride Counts by Day of Week and Membership Type\n",
    "\n",
    "from pyspark.sql.functions import date_format, dayofweek, count, sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_table = df.groupBy(date_format('started_at', 'EEEE').alias('day'), 'member_casual')\\\n",
    "                .agg(count('*').alias('ride_count'))\\\n",
    "                .groupBy('day').pivot('member_casual')\\\n",
    "                .agg(sum('ride_count'))\\\n",
    "                .orderBy(dayofweek('day'))\n",
    "\n",
    "pivot_table_pd = pivot_table.toPandas()\n",
    "\n",
    "pivot_table_pd['member'] = pivot_table_pd['member'].astype(float)\n",
    "pivot_table_pd['casual'] = pivot_table_pd['casual'].astype(float)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(pivot_table_pd['day'], pivot_table_pd['casual'], label='Casual')\n",
    "plt.bar(pivot_table_pd['day'], pivot_table_pd['member'], bottom=pivot_table_pd['casual'], label='Member')\n",
    "plt.title('Ride Counts by Day of Week and Membership Type')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Ride Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0c11ba-0624-44e0-a6b6-11b78f7721e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rideable Type and Customer Type\n",
    "\n",
    "ride_counts = df.groupby(['rideable_type', 'member_casual']).count().select(['rideable_type', 'member_casual', 'count']).toPandas()\n",
    "\n",
    "pivot_table = ride_counts.pivot(index='member_casual', columns='rideable_type', values='count')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_table.plot(kind='bar', stacked=True)\n",
    "plt.title('Rideable Type and Customer Type')\n",
    "plt.xlabel('Customer Type')\n",
    "plt.ylabel('Number of Rides')\n",
    "plt.legend(title='Rideable Type', loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354baf9e-c22a-4b18-8ddb-031705bb542b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q2 What are the daily trip trends for your given station?\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "df_q2 = spark.read.format(\"delta\").load(delta_path)\n",
    "df_q2 = df_q2.withColumn(\"start_date\", date_format(\"started_at\", \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "daily_trips = df_q2.groupBy(\"start_date\").count()\n",
    "daily_trips = daily_trips.orderBy(\"start_date\")\n",
    "\n",
    "display(daily_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8b9f12-b1ea-4836-aa7a-0f296607d833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# line chart for daily bike trip trends\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(daily_trips.toPandas(), x=\"start_date\", y=\"count\", title=\"Daily Bike Trip Trends\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec8daed-3315-48e6-ac45-8f3c79ae99c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# bar graph\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=daily_trips.toPandas()['start_date'], y=daily_trips.toPandas()['count'], \n",
    "                             marker=dict(color='blue'))])\n",
    "fig.update_layout(title='Total Trips by Day', xaxis_title='Day', yaxis_title='Trips')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c43c7ea-f2c1-4579-bd9f-433c9c17a55c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dates with zero rides or missing data\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_date\n",
    "start_date = \"2021-11-01\"\n",
    "end_date = \"2023-02-28\"\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D').strftime('%Y-%m-%d').tolist()\n",
    "df = df.withColumn(\"start_date\", date_format(\"started_at\", \"yyyy-MM-dd\").cast(\"date\"))\n",
    "start_date_list = [str(row.start_date) for row in df.select(\"start_date\").collect()]\n",
    "uncommon_elements = list(set(date_range).symmetric_difference(set(start_date_list)))\n",
    "uncommon_elements = sorted(uncommon_elements, key=lambda d: datetime.datetime.strptime(d, '%Y-%m-%d'))\n",
    "print(f\"Total {len(uncommon_elements)} days with zero rides\")\n",
    "for i in uncommon_elements:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee73b82-d13a-4f6e-ae21-ece97b811801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "import datetime\n",
    "# Create a list to store the holiday dates\n",
    "holiday_dates = []\n",
    "\n",
    "# Loop through the years 2021 to 2023\n",
    "for year in range(2021, 2024):\n",
    "\n",
    "    # Loop through all the days in the current year\n",
    "    for month in range(1, 13):\n",
    "        for day in range(1, 32):\n",
    "            try:\n",
    "                date = datetime.date(year, month, day)\n",
    "                # Check if the current date is a Saturday or Sunday\n",
    "                if date.weekday() in [5, 6]:\n",
    "                    holiday_dates.append(date.strftime('%Y-%m-%d'))\n",
    "            except ValueError:\n",
    "                # Skip invalid dates\n",
    "                pass\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "# holidays = [\"2021-11-25\", \"2021-12-25\", \"2022-01-01\", \"2022-01-17\", \"2022-02-21\"]\n",
    "# holidays = [datetime.datetime.strptime(h, '%Y-%m-%d').date() for h in holidays]\n",
    "# print(holidays)\n",
    "\n",
    "df_q2 = spark.read.format(\"delta\").load(delta_path)\n",
    "# df_q2 = df_q2.withColumn(\"start_date\", date_format(\"started_at\", \"yyyy-MM-dd\").cast(\"date\"))\n",
    "\n",
    "# Define a UDF to check if a date is a holiday\n",
    "def is_holiday(date):\n",
    "  return str(date) in holiday_dates\n",
    "is_holiday_udf = udf(is_holiday, BooleanType())\n",
    "\n",
    "# Add a column to indicate whether each date is a holiday or not\n",
    "df_q2 = df_q2.withColumn(\"is_holiday\", is_holiday_udf(col(\"date\")))\n",
    "\n",
    "# Group the data by date and holiday/non-holiday\n",
    "daily_trips = df_q2.groupBy(\"date\", \"is_holiday\").count()\n",
    "daily_trips = daily_trips.orderBy(\"date\")\n",
    "\n",
    "# Split the data into holiday and non-holiday data frames\n",
    "daily_trips_holiday = daily_trips.filter(col(\"is_holiday\") == True)\n",
    "daily_trips_nonholiday = daily_trips.filter(col(\"is_holiday\") == False)\n",
    "\n",
    "# Display the daily trip trends for holidays and non-holidays\n",
    "print(\"Daily Trip Trends for Holidays:\")\n",
    "display(daily_trips_holiday)\n",
    "\n",
    "print(\"Daily Trip Trends for Non-Holidays:\")\n",
    "display(daily_trips_nonholiday)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa1646f-14e3-4647-99c8-835808239a59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_trips_nonholiday = daily_trips_nonholiday.filter(col(\"is_holiday\") == False).agg({\"count\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "avg_trips_holiday = daily_trips_holiday.filter(col(\"is_holiday\") == True).agg({\"count\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "print(\"Average trips per day for non-holidays (excluding holidays):\", int(avg_trips_nonholiday))\n",
    "print(\"Average trips per day for holidays:\", int(avg_trips_holiday))\n",
    "\n",
    "percentage_change = (avg_trips_holiday - avg_trips_nonholiday) / avg_trips_nonholiday * 100\n",
    "\n",
    "print(\"Percentage change in the average number of trips per day due to holidays:\", percentage_change, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa473f6-7bbd-4554-bc85-01a149e72b87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q4: How does weather affect the daily/hourly trend of system use?\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import to_timestamp, from_unixtime,desc\n",
    "delta_table = \"dbfs:/FileStore/tables/G06/bronze/nyc_weather_history/\"\n",
    "weather_df_with_datetime = spark.read.format(\"delta\").load(delta_table)\n",
    "weather_df_with_datetime=weather_df_with_datetime.toPandas()\n",
    "# check for duplicates in two columns\n",
    "duplicates = weather_df_with_datetime[weather_df_with_datetime[['date', 'time']].duplicated()]\n",
    "# Drop the duplicates in the weather dataframe\n",
    "weather_df_with_datetime = weather_df_with_datetime.drop_duplicates(subset=['date', 'time'])\n",
    "\n",
    "# display(weather_df_with_datetime.orderBy(desc(\"date\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33dc9d08-ead5-4a4a-a26e-cacabfbdb30a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, count,hour,round,date_format,to_timestamp,when,lit,concat,ceil,col\n",
    "import pandas as pd\n",
    "\n",
    "delta_path = \"dbfs:/FileStore/tables/G06/historic_bike_trip_g06/\"\n",
    "\n",
    "rides_df = spark.read.format(\"delta\").load(delta_path)\n",
    "display(rides_df.limit(5))\n",
    "rides_df=rides_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0beaad-2864-4910-b228-85cbc1b034ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(weather_df_with_datetime,rides_df, how='left', on=['date', 'time'])\n",
    "spark_df = spark.createDataFrame(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c030920-598f-484a-befc-59319ad981b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "325cad2c-adbc-4984-a9bb-48a15d7e275a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803dd68c-e51f-4490-b52c-1e274c2ec40f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64200106-d10f-40c1-be20-cbb601a732f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(spark_df.select('main').distinct())\n",
    "sf=spark_df.filter(col('main').isNull())\n",
    "display(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9372a69-614e-498a-a601-cddc018aae56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "rides_by_weather = spark_df.groupBy('date','main').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "display(rides_by_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fc4e941-5ae2-4eae-8f09-c75bf6ad446d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create a bar chart of ride counts by weather main category\n",
    "rides_by_weather = spark_df.groupBy('main').agg(count('ride_id').alias('ride_count')).toPandas()\n",
    "\n",
    "fig = px.bar(rides_by_weather, x='main', y='ride_count', title='Ride counts by weather category')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b90e70da-c41f-4efa-a9e2-af1bc08d30b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Round off the feels_like column to 2 decimal places\n",
    "spark_df = spark_df.withColumn('feels_like', round(col('feels_like'), 0))\n",
    "rides_by_weather = spark_df.groupBy('feels_like').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "display(rides_by_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f9920ab-e538-479c-a4f4-ab4cd05935cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('temp_category', \n",
    "                               when(col('feels_like') > 298, 'High')\n",
    "                               .when(col('feels_like').between(278, 298), 'Medium')\n",
    "                               .otherwise('Low'))\n",
    "display(spark_df.limit(5))\n",
    "rides_by_weather = spark_df.groupBy('temp_category').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "display(rides_by_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ce27953-b9e3-437d-9e6e-2316bca0de2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df.select('main').distinct())\n",
    "sf=spark_df.filter(col('main').isNull())\n",
    "display(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e64c96-5ad3-42fc-8c27-f401d1ae596f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rides_by_weather = spark_df.groupBy('date', 'main').agg(count('ride_id').alias('ride_count'))\n",
    "display(rides_by_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73cdae62-0f3a-454e-b343-7a0a2bffa446",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the weather conditions of interest\n",
    "good_weather_conditions = ['Clear', 'Clouds']\n",
    "bad_weather_conditions = ['Rain', 'Snow', 'Thunderstorm','Mist','Smoke','Drizzle','Haze','Fog']\n",
    "\n",
    "# Create new columns to identify good/bad weather days\n",
    "rides_by_weather = rides_by_weather.withColumn('good_weather',\n",
    "                                               when(rides_by_weather['main'].isin(good_weather_conditions), 1)\n",
    "                                               .otherwise(0))\n",
    "rides_by_weather = rides_by_weather.withColumn('bad_weather',\n",
    "                                               when(rides_by_weather['main'].isin(bad_weather_conditions), 1)\n",
    "                                               .otherwise(0))\n",
    "\n",
    "# Sum the ride counts for each weather condition and date\n",
    "good_weather_rides = rides_by_weather.filter(rides_by_weather['good_weather'] == 1)\\\n",
    "                                      .groupBy('date')\\\n",
    "                                      .agg(sum('ride_count').alias('good_weather_rides'))\n",
    "bad_weather_rides = rides_by_weather.filter(rides_by_weather['bad_weather'] == 1)\\\n",
    "                                     .groupBy('date')\\\n",
    "                                     .agg(sum('ride_count').alias('bad_weather_rides'))\n",
    "\n",
    "# Join the ride counts by weather condition and date\n",
    "rides_by_weather_trend = good_weather_rides.join(bad_weather_rides, 'date', 'outer')\\\n",
    "                                           .orderBy('date')\n",
    "\n",
    "display(rides_by_weather_trend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f5aad6-4f72-4ea4-a002-df7cc2eb61d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data from DataFrame\n",
    "dates = rides_by_weather_trend.select('date').collect()\n",
    "good_weather_rides = rides_by_weather_trend.select('good_weather_rides').collect()\n",
    "bad_weather_rides = rides_by_weather_trend.select('bad_weather_rides').collect()\n",
    "\n",
    "# Create a line chart with two lines\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates, good_weather_rides, label='Good Weather Rides')\n",
    "ax.plot(dates, bad_weather_rides, label='Bad Weather Rides')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Rides')\n",
    "ax.set_title('Ride Counts by Weather Condition and Date')\n",
    "\n",
    "# Add legend and display the chart\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db49a4a0-45a0-4ba2-aa03-7d890536669f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract data from DataFrame\n",
    "dates = rides_by_weather_trend.select('date').collect()\n",
    "good_weather_rides = rides_by_weather_trend.select('good_weather_rides').collect()\n",
    "bad_weather_rides = rides_by_weather_trend.select('bad_weather_rides').collect()\n",
    "\n",
    "# Create a scatter plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(dates, good_weather_rides, label='Good Weather Rides')\n",
    "ax.scatter(dates, bad_weather_rides, label='Bad Weather Rides')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Rides')\n",
    "ax.set_title('Ride Counts by Weather Condition and Date')\n",
    "\n",
    "# Add legend and display the chart\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c7a0397-6afb-403b-9a56-90c9bf8abcc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate the ride counts by weather condition and date\n",
    "rides_by_weather = spark_df.groupBy('date', 'main').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for plotting\n",
    "rides_by_weather_pd = rides_by_weather.toPandas()\n",
    "\n",
    "# Create a line chart of ride counts by weather condition over time\n",
    "plt.figure(figsize=(20, 12))\n",
    "for weather in rides_by_weather_pd['main'].unique():\n",
    "    rides_by_weather_pd[rides_by_weather_pd['main'] == weather].plot(x='date', y='ride_count', label=weather)\n",
    "plt.title('Ride Counts by Weather Condition over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Ride Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52fdbf22-ea3e-4ddc-ac92-afe2c94353a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Aggregate the ride counts by weather condition and date\n",
    "rides_by_weather = spark_df.groupBy('date', 'main').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "# Filter the data to include only the first 10 days\n",
    "start_date = rides_by_weather.select('date').orderBy('date').first()[0]\n",
    "end_date = pd.to_datetime(start_date) + pd.Timedelta(days=10)\n",
    "rides_by_weather = rides_by_weather.filter((rides_by_weather['date'] >= start_date) & (rides_by_weather['date'] < end_date))\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame for plotting\n",
    "rides_by_weather_pd = rides_by_weather.toPandas()\n",
    "\n",
    "# Create a line chart of ride counts by weather condition over time using Plotly\n",
    "fig = px.line(rides_by_weather_pd, x='date', y='ride_count', color='main',\n",
    "              title='Ride Counts by Weather Condition over Time')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d9b7c3-5329-42cb-b53b-b70423e8b41d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, when, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Calculate the total ride count for each day and weather condition\n",
    "rides_by_weather = spark_df.groupBy('date', 'main').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "# Calculate the total ride count for each day\n",
    "total_rides_by_day = rides_by_weather.groupBy('date').agg(sum('ride_count').alias('total_rides'))\n",
    "\n",
    "# Calculate the percentage of rides for each weather condition for each day\n",
    "rides_by_weather_percent = rides_by_weather.join(total_rides_by_day, 'date')\\\n",
    "                                           .withColumn('percent_rides', (rides_by_weather['ride_count'] / total_rides_by_day['total_rides']) * 100)\n",
    "\n",
    "# Calculate the change in percentage of rides for each weather condition compared to the previous day\n",
    "w = Window.partitionBy('main').orderBy('date')\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('prev_percent_rides', lag('percent_rides', 1).over(w))\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('percent_change', (rides_by_weather_percent['percent_rides'] - rides_by_weather_percent['prev_percent_rides']) / rides_by_weather_percent['prev_percent_rides'] * 100)\n",
    "\n",
    "# Display the results\n",
    "display(rides_by_weather_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37cd35fa-8919-4297-8f03-86dd6ab9ba02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate the overall average percentage change of rides by weather condition\n",
    "avg_percent_change = rides_by_weather_percent.groupBy('main')\\\n",
    "                                             .agg(avg('percent_change').alias('avg_percent_change'))\n",
    "\n",
    "# Display the results\n",
    "display(avg_percent_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0040b71-6036-45fe-b921-b85675f60274",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead, col\n",
    "\n",
    "# Calculate the percentage change for each day\n",
    "rides_by_weather_percent = rides_by_weather.join(total_rides_by_day, 'date')\\\n",
    "                                           .withColumn('percent_rides', (rides_by_weather['ride_count'] / total_rides_by_day['total_rides']) * 100)\n",
    "\n",
    "# Calculate the weather condition for the previous day\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('prev_main', lead(col('main')).over(Window.orderBy('date')))\n",
    "\n",
    "# Filter the rows where the weather condition changes from one day to the next\n",
    "filtered_rides = rides_by_weather_percent.filter(col('prev_main') != col('main'))\n",
    "\n",
    "# Calculate the average percentage change for these filtered rows\n",
    "avg_percent_change = filtered_rides.agg({'percent_rides': 'avg'}).collect()[0][0]\n",
    "\n",
    "print('Average percentage change of rides when the weather changes:', avg_percent_change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a62cfb-4e07-472d-b0ea-01ee89ba617f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, when, count, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate the total ride count for each day and weather condition\n",
    "rides_by_weather = spark_df.groupBy('date', 'main').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "# Calculate the total ride count for each day\n",
    "total_rides_by_day = rides_by_weather.groupBy('date').agg(sum('ride_count').alias('total_rides'))\n",
    "\n",
    "# Calculate the percentage of rides for each weather condition for each day\n",
    "rides_by_weather_percent = rides_by_weather.join(total_rides_by_day, 'date')\\\n",
    "                                           .withColumn('percent_rides', (rides_by_weather['ride_count'] / total_rides_by_day['total_rides']) * 100)\n",
    "\n",
    "# Calculate the change in percentage of rides for each weather condition compared to the previous day\n",
    "w = Window.partitionBy('main').orderBy('date')\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('prev_percent_rides', lag('percent_rides', 1).over(w))\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('percent_change', (rides_by_weather_percent['percent_rides'] - rides_by_weather_percent['prev_percent_rides']) / rides_by_weather_percent['prev_percent_rides'] * 100)\n",
    "\n",
    "# Use a CTE to filter only the rows where the weather condition changed from one day to the next\n",
    "rides_by_weather_percent.createOrReplaceTempView('rides_by_weather_percent_table')\n",
    "rides_by_weather_percent_change = spark.sql('''\n",
    "WITH temp_table AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        LAG(main) OVER (ORDER BY date) AS prev_main\n",
    "    FROM rides_by_weather_percent_table\n",
    ")\n",
    "SELECT \n",
    "    temp_table.date,\n",
    "    temp_table.main AS from_weather,\n",
    "    temp_table.prev_main AS to_weather,\n",
    "    AVG(temp_table.percent_change) AS avg_percent_change\n",
    "FROM \n",
    "    temp_table\n",
    "WHERE \n",
    "    temp_table.main != temp_table.prev_main\n",
    "GROUP BY \n",
    "    temp_table.date, \n",
    "    temp_table.main,\n",
    "    temp_table.prev_main\n",
    "ORDER BY \n",
    "    temp_table.date, \n",
    "    temp_table.main,\n",
    "    temp_table.prev_main\n",
    "''')\n",
    "\n",
    "# Display the results\n",
    "rides_by_weather_percent_change.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd580f26-f35c-4d1c-ac23-c3c49e5e18c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, when, count, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calculate the total ride count for each day and weather condition\n",
    "rides_by_weather = spark_df.groupBy('date', 'main').agg(count('ride_id').alias('ride_count'))\n",
    "\n",
    "# Calculate the total ride count for each day\n",
    "total_rides_by_day = rides_by_weather.groupBy('date').agg(sum('ride_count').alias('total_rides'))\n",
    "\n",
    "# Calculate the percentage of rides for each weather condition for each day\n",
    "rides_by_weather_percent = rides_by_weather.join(total_rides_by_day, 'date')\\\n",
    "                                           .withColumn('percent_rides', (rides_by_weather['ride_count'] / total_rides_by_day['total_rides']) * 100)\n",
    "\n",
    "# Calculate the change in percentage of rides for each weather condition compared to the previous day\n",
    "w = Window.partitionBy('main').orderBy('date')\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('prev_percent_rides', lag('percent_rides', 1).over(w))\n",
    "rides_by_weather_percent = rides_by_weather_percent.withColumn('percent_change', (rides_by_weather_percent['percent_rides'] - rides_by_weather_percent['prev_percent_rides']) / rides_by_weather_percent['prev_percent_rides'] * 100)\n",
    "\n",
    "# Use a CTE to filter only the rows where the weather condition changed from one day to the next\n",
    "rides_by_weather_percent.createOrReplaceTempView('rides_by_weather_percent_table')\n",
    "rides_by_weather_percent_change = spark.sql('''\n",
    "WITH temp_table AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        LAG(main) OVER (ORDER BY date) AS prev_main\n",
    "    FROM rides_by_weather_percent_table\n",
    ")\n",
    "SELECT \n",
    "    temp_table.main AS from_weather,\n",
    "    temp_table.prev_main AS to_weather,\n",
    "    AVG(temp_table.percent_change) AS avg_percent_change\n",
    "FROM \n",
    "    temp_table\n",
    "WHERE \n",
    "    temp_table.main != temp_table.prev_main\n",
    "GROUP BY \n",
    "    temp_table.main,\n",
    "    temp_table.prev_main\n",
    "ORDER BY \n",
    "    temp_table.main,\n",
    "    temp_table.prev_main\n",
    "''')\n",
    "\n",
    "# Display the results\n",
    "rides_by_weather_percent_change.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27262e8f-1d93-4201-9bee-fc4523aaa13b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rides_by_weather_percent_change.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d7516b-b2c8-4d21-a53e-d0c9499b4380",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(rides_by_weather_percent.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c6fb03-7d38-4497-8289-27bd6f197fb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rides_weather_df = rides_by_weather_percent.toPandas()\n",
    "rides_weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15ed9575-322a-4c84-b10f-ac49ca5d0e56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import plotly.express as px\n",
    "\n",
    "dec_rides_weather = rides_by_weather_percent.filter((F.col('date') >= '2021-12-01') & (F.col('date') <= '2021-12-31'))\n",
    "\n",
    "dec_rides_weather_df = dec_rides_weather.toPandas()\n",
    "\n",
    "fig = px.bar(dec_rides_weather_df['percent_change'], dec_rides_weather_df['date'], color=dec_rides_weather_df['main'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbaaec64-588c-469c-a5b9-c5297fe2feed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 782324349803110,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "G06_EDA (1)",
   "notebookOrigID": 4119671058939265,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
